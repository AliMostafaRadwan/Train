{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 02:08:33.448448: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-29 02:08:36.544281: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-29 02:08:36.784408: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /tmp/.mount_CursorQ9GjqY/usr/lib:\n",
      "2023-06-29 02:08:36.784437: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-29 02:08:42.170895: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /tmp/.mount_CursorQ9GjqY/usr/lib:\n",
      "2023-06-29 02:08:42.171133: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /tmp/.mount_CursorQ9GjqY/usr/lib:\n",
      "2023-06-29 02:08:42.171147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          idx                                           document  \\\n",
      "29193  9993_1  قد تصبح لديك مناطق من الماء الآسن إذا تشكلت كت...   \n",
      "29194  9993_2  . يجب ألا تنمو الطحالب إذا بقيت مطلعًا على كيم...   \n",
      "29195  9994_0  عند الانتهاء من تناول الوجبة وتدوين الملاحظات،...   \n",
      "29196  9994_1  يجب أن تحصل على نفس التجربة التي سيحصل عليها أ...   \n",
      "29197  9994_2  عندما يصل الطبق إلى طاولتك، دوّن ملاحظتك عن مظ...   \n",
      "\n",
      "                                                 summary  \n",
      "29193  قم بتحسين الدورة للتعامل مع مع البقع الصغيرة م...  \n",
      "29194   حافظ على ماء حمام السباحة أضف مبيد الطحالب كو...  \n",
      "29195  قم ببعض البحث. افتتح التقييم بجملة جذابة. صف 3...  \n",
      "29196  تجنب إخبار العاملين بالمطعم أنك ناقد أو مقيّم ...  \n",
      "29197  لاحظ طريقة تقديم الطبق. استمتع بأول قضمات. اكت...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from nltk.stem import ISRIStemmer\n",
    "from pathlib import Path\n",
    "\n",
    "stemmer = ISRIStemmer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "df = pd.read_csv('WikiLingua_cleaned.csv')\n",
    "\n",
    "print(df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idx                                           document  \\\n",
      "0  0_0  يكون سعر الفاكهة والخضراوات في موسم إنباتها أق...   \n",
      "1  0_1  الأطعمة الصحية ليست باهظة الثمن بالضرورة، بل ف...   \n",
      "2  0_2  استفد من حديقتك المنزلية أو أصيص الزرع الصغير ...   \n",
      "3  0_3  تساعدك الخطط المسبقة في كل نواحي حياتك على وضع...   \n",
      "4  1_0  نظرًا لأن السبب الرئيسي لضغط العين هو أن ثقافة...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  تناول الفاكهة والخض راوات موسم ها تعرف اسعار ا...   \n",
      "1  فضل خيارات الاطعمة الار خص ثمنا تب اطا استهلاك...   \n",
      "2  از رع يمكنك خضر اوا ت وفا كهة اط ه بنفسك وجبات...   \n",
      "3  خطط مسبقا لوج بات ك الرييسية لمدة اسبوع التزم ...   \n",
      "4               قل ل وقت التعرض للش اشات اذهب الطبيب   \n",
      "\n",
      "                                                text  \n",
      "0  سعر الفاكهة والخض راوات موسم انبا تها اقل غيره...  \n",
      "1  الاطعمة الصحية ليست باه ظة الثمن بالضرورة حقيق...  \n",
      "2  استف د حد يق تك المنزلية اص يص الزرع الصغير ال...  \n",
      "3  تساعدك الخطط المسب قة نواحي حياتك وضع تصور كام...  \n",
      "4  نظرا لان السبب الرييسي لض غط العين ثقافة تفرض ...  \n"
     ]
    }
   ],
   "source": [
    "with open(Path(\"Stopwords/Stopwords_List.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    arabic_stopwords = set(f.read().splitlines())\n",
    "\n",
    "def preprocess(text):\n",
    "    if isinstance(text,str):\n",
    "        text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]+', ' ', text)\n",
    "        tokens = tokenizer.tokenize(text, add_special_tokens=False)\n",
    "        tokens = [token for token in tokens if token not in arabic_stopwords]\n",
    "        # tokens = [stemmer.stem(token) for token in tokens]\n",
    "        tokens = [token.translate(str.maketrans(\"\", \"\", string.punctuation)) for token in tokens]\n",
    "        tokens = [token for token in tokens if not token.isdigit()]\n",
    "        tokens = [token for token in tokens if token]\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "texts = df['document'].tolist()\n",
    "summaries = df['summary'].tolist()\n",
    "\n",
    "text_batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "summary_batches = [summaries[i:i+batch_size] for i in range(0, len(summaries), batch_size)]\n",
    "\n",
    "processed_texts = []\n",
    "processed_summaries = []\n",
    "\n",
    "with open(\"Preprocessed_Texts/preprocessed_texts.txt\", \"w\", encoding=\"utf-8\") as text_file, \\\n",
    "        open(\"Preprocessed_Texts/preprocessed_summaries.txt\", \"w\", encoding=\"utf-8\") as summary_file:\n",
    "\n",
    "    for i, (text_batch, summary_batch) in enumerate(zip(text_batches, summary_batches)):\n",
    "        batch_texts = [preprocess(text) for text in text_batch]\n",
    "        batch_summaries = [preprocess(summary) for summary in summary_batch]\n",
    "\n",
    "        processed_texts.extend(batch_texts)\n",
    "        processed_summaries.extend(batch_summaries)\n",
    "\n",
    "        for j, text in enumerate(batch_texts):\n",
    "            text_file.write(f\"Text {i * batch_size + j + 1}: {text}\\n\")\n",
    "\n",
    "        for j, summary in enumerate(batch_summaries):\n",
    "            summary_file.write(f\"Summary {i * batch_size + j + 1}: {summary}\\n\")\n",
    "\n",
    "df['text'] = processed_texts[:len(df)]\n",
    "df['summary'] = processed_summaries[:len(df)]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 02:10:15.102287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-29 02:10:15.161095: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /tmp/.mount_CursorQ9GjqY/usr/lib:\n",
      "2023-06-29 02:10:15.161364: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /tmp/.mount_CursorQ9GjqY/usr/lib:\n",
      "2023-06-29 02:10:15.161434: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-06-29 02:10:15.161987: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "train_texts = df['document'][:train_size].tolist()\n",
    "train_summaries = df['summary'][:train_size].tolist()\n",
    "test_texts = df['document'][train_size:].tolist()\n",
    "test_summaries = df['summary'][train_size:].tolist()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)[0]\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def evaluate(texts, summaries):\n",
    "    rouge = tf.keras.metrics.Mean()\n",
    "    for i in range(len(texts)):\n",
    "        inputs = tokenizer.encode(texts[i], max_length=512, truncation=True, padding='max_length', return_tensors='tf')\n",
    "        targets = tokenizer.encode(summaries[i], max_length=128, truncation=True, padding='max_length', return_tensors='tf')\n",
    "        predictions = model(inputs)[0]\n",
    "        rouge.update_state(targets, predictions)\n",
    "    return rouge.result().numpy()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(train_texts), batch_size):\n",
    "        batch_texts = train_texts[i:i+batch_size]\n",
    "        batch_summaries = train_summaries[i:i+batch_size]\n",
    "        inputs = tokenizer.batch_encode_plus(batch_texts, max_length=512, truncation=True, padding='max_length', return_tensors='tf')\n",
    "        targets = tokenizer.batch_encode_plus(batch_summaries, max_length=128, truncation=True, padding='max_length', return_tensors='tf')\n",
    "        loss = train_step(inputs['input_ids'], targets['input_ids'])\n",
    "        print(f\"Epoch {epoch + 1}, Batch{int(i/batch_size) + 1} Loss: {loss:.4f}\")\n",
    "\n",
    "    rouge_score = evaluate(test_texts, test_summaries)\n",
    "    print(f\"Epoch {epoch + 1} ROUGE: {rouge_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
